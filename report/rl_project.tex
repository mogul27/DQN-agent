\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading rl_project.

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{rl_project}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{rl_project}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\usepackage{amsmath}        % for pseudo code
\usepackage{algorithm}      % make pseudo code easier
\usepackage[noend]{algpseudocode}




% Give your project report an appropriate title!

\title{RL Project}


% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus
  \\
  Department of Computer Science\\
  University of Bath\\
  Bath, BA2 7AY \\
  \texttt{hippo@bath.ac.uk} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}

\maketitle

\section{Problem Definition}

% A clear, precise and concise description of your chosen problem, including the states, actions, transition dynamics, and the reward function. You will lose marks for an unclear, incorrect, or incomplete problem definition.

\section{Background}

%A discussion of reinforcement learning methods that may be effective at solving your chosen problem, their strengths and weaknesses for your chosen problem, and any existing results in the scientific literature (or publicly available online) on your chosen problem or similar problems.

\subsection{DQN (Mirco)}

\subsection{A2C (Chris)}

\subsection{Async (Peter)}

Mnih et al (2016) present asynchronous variants of reinforcement learning algorithms including both Q-Learning and Actor-Critic (A3C).
The paper claims the asynchronous variants can produce excellent results on single multi-core CPUs with training times of just a few hours. 

Unlike DQN, the asynchronous methods do not require an experience replay memory because multiple agents working in parallel means that the data is fragmented and is sufficiently decorellated.

The results published by Mnih et al (2016) are impressive and show that after 10 hours training with breakout, Async Q-Learning scored 240 and A3C scored 460, whilst DQN only managed 25.

TODO : Weaknesses? Check publications for A3C vs A2C, I've seen comments that say A3C does not improve on A2C, but can't find anything definitive. ??


\section{Method}
%A description of the method(s) used to solve your chosen problem, an explanation of how these methods work (in your own words), and an explanation of why you chose these specific methods.
\subsection{DQN (Mirco)}

\subsection{A2C (Chris)}

\subsection{Async Q-Learning (Peter)}

Async Q-Learning was chosen as it was similar to DQN, but promised improved learning in a shorter time.
Pseudo code is shown in \nameref{async_q_pseudo}

Like DQN, Async Q-Learning uses an e-greedy policy based on a value network to select actions to take and then uses a target network to get an action value used to calculate the loss.
The weights in the target network are occasionally reset to match the value network.

Where Async Q-Learning differs from DQN is that it runs multiple worker agents in separate processes which share the value and target networks.
Each worker agent keeps a local copy of the value and target networks to calculate the losses and gradients which it then applies to the shared networks every few steps. 
Once the worker agent has updated the shared network, it resets the weights in its own copies to match shared ones.
As the worker agent takes a number of steps before updating the shared network, the updates are done like mini-batches and the chances of one worker agent overwritting changes being made by another are reduced.

Having multiple worker agents running at the same time, and the non-deterministic nature of breakout means that different areas of the game will be explored at the same time. 
The different worker agents were also given different epsilon values for their e-greedy policies to further increase the variety of exploration.

The e-greedy policies for the worker agents all started with an epsilon of 1.0, which decayed down to 0.05, 0.1 or 0.1 after 1,000 episodes.

A controller process handles the creation of multiple workers as different processes. 
The controller gives each worker a queue which allows communication between controller and workers. 
Each worker starts the atari environment and plays games of breakout. 
An episode is a complete game, and at the end of each episode the worker sends some details to the controller.

The controller keeps track of the total number of episodes completed by all the workers, and uses another process to play games with a policy based on the shared value network with an epsilon of 0.01 to provide a measure of how well the policy performs. 
The graphs in the results section use 

TODO : Maybe a diagram of the different processes would help here?


\section{Results}

%result comparison between the 3 approaches.
%how quickly each agent learns
%how well do they perform is absolte terms
%how do they compare  with respect to a human player

\subsection{DQN (Mirco)}
\subsection{A2C (Chris)}
\subsection{Async Q-Learning (Peter)}

TODO : Add images for the training using asynch Q-Learning. 
Show score against number of frames processed.

TODO : add a graph showing 10 hours training of asynch Q-Learning ?

\section{Discussion}
    
%An evaluation of how well you solved your chosen problem.

\subsection{DQN (Mirco)}
\subsection{A2C (Chris)}
\subsection{Async Q-Learning (Peter)}

Initial results were encouraging, but the performance tended to level off quite quickly and often started to decrease. 

Asynch Q-Learning did not produce results in the time scales mentioned by Mnih et al (2016).
However, matching the scores on the data efficiency and training time graphs from Mnih et al (2016) show that they completed approx 25 epochs in 10 hours.
A single epoch corresponds to 4 million frames giving 100 million frames in 10 hours. 
In contrast our implementation for asynch Q-Learning only managed to process 5 million frames in 10 hours, so approximately 20 times slower.

TODO : put these numbers into a suitable table ??

\section{Future Work}

%What other techniques can be used to improve further the performances of the 3 agents.

\section{Personal Experience}

%A discussion of your personal experience with the project, such as difficulties or pleasant surprises you encountered while completing it.

\section*{References}
\small

Mnih, V., et al., 2016. Asynchronous Methods for Deep Reinforcement Learning. ArXiv, [Online] 1602.01783.
Available from: https://arxiv.org/pdf/1602.01783.pdf [Accessed 28 Dec 2022]

\normalsize
\newpage
\section*{Appendices}
%If you have additional content that you would like to include in the appendices, please do so here.
%There is no limit to the length of your appendices, but we are not obliged to read them in their entirety while marking. The main body of your report should contain all essential information, and content in the appendices should be clearly referenced where it's needed elsewhere.
\subsection*{Appendix A: DQN pseudo-code}
\label{dqn_pseudo}
Taken from the course notes.
\begin{algorithmic}[1]
\State Initialise replay memory $D$ to capacity $N$
\State Initialise action-value network $\hat{q}_{1}$ with parameters $\boldsymbol{\theta}_{1} \in \mathbb{R}^{d}$ arbitrarily
\State Initialise target action-value network $\hat{q}_{2}$ with parameters $\boldsymbol{\theta}_{2}=\boldsymbol{\theta}_{1}$
\For{each episode}
    \State $\text{Initialise } S$
    \For{for each step of episode}
        \State Choose action $A$ in state $S$ using policy derived from  $\hat{q}_{1}\left(S, \cdot, \theta_{1}\right)$
        \State Take action $A$ observe reward $R$ and next-state $S^{\prime}$ 
        \State Store transition $\left(S, A, R, S^{\prime}\right)$ in $D$
        \For{each transition $\left(S_{j}, A_{j}, R_{j}, S_{j}^{\prime}\right)$ in minibatch sampled from $D$}
            \State $y= \begin{cases}R_{j} & \text { if } S_{j}^{\prime} \text { is terminal } \\ R_{j}+\gamma \max _{a^{\prime}} \hat{q}_{2}\left(S_{j}^{\prime}, a^{\prime}, \boldsymbol{\theta}_{2}\right) & \text { otherwise }\end{cases}$
            \State $\hat{y}=\hat{q}_{1}\left(S_{j}, A_{j}, \boldsymbol{\theta}_{1}\right)$
            \State Perform gradient descent step $\nabla_{\theta_{1}} L_{\delta}(y, \hat{y})$
        \EndFor
    \State Every $C$ time-steps, update $\boldsymbol{\theta}_{2}=\boldsymbol{\theta}_{1}$
    \EndFor
\EndFor
\end{algorithmic}

\subsection*{Appendix B: A2C pseudo-code}
\label{async_a2c_pseudo}

\begin{algorithmic}[1]
\State TODO
\end{algorithmic}


\subsection*{Appendix C: Async Q-Learning pseudo-code}
\label{async_q_pseudo}

From Mnih et al (2016)

\begin{algorithmic}[1]
\State \textit{// Assume global shared  $\theta$, $\theta^{-}$, and counter $T = 0$}
\State Initialise thread step counter $t \gets 0$
\State Initialise target network weights $\theta^{-} \gets \theta$
\State Initialise network gradients $d\theta \gets 0$
\State Get initial stats $s$
\While{$T <= T_{\max}$}
    \State Take action $a$ with $\epsilon$-greedy policy based on $Q\left(s,a;\theta\right)$
    \State Receive new state $s^{\prime}$ and reward $r$
    \State $y= \begin{cases}r & \text { for terminal } s^{\prime} \\ r + \gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}, \theta^{-}\right) & \text { for non terminal } s^{\prime}\end{cases}$
    \State Accumulate gradients wrt $\theta$: $d\theta \gets d\theta + \frac{\delta\left( y-Q\left(s, a, \theta\right)\right)^{2}}{\delta\theta}$
    \State $s = s^{\prime}$
    \State $T \gets T + 1$ and $t \gets t + 1$
    \If{$T$ mod $I_{target} == 0$}
        \State Update the target network $\theta^{-} \gets \theta$
    \EndIf
    \If{$t$ mod $I_{AsyncUpdate} == 0$ or $s$ is terminal}
        \State Perform asynchronous update ot $\theta$ using $d\theta$
        \State Clear gradients $d\theta \gets 0$
    \EndIf
\EndWhile

\end{algorithmic}


\subsection*{Appendix D: }

\end{document}
