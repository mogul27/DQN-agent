import multiprocessing as mp
from queue import Empty
import os
from pathlib import Path
import time
import random
from collections import deque
from statistics import mean
import matplotlib.pyplot as plt

import gym

from dqn_utils import Options, Timer

# import keras
from keras.models import Sequential
from keras import Model
from keras.layers import Input, Dense, Conv2D, Flatten
from keras.losses import Huber
from keras.optimizers import Adam

import numpy as np


class EGreedyPolicy:
    """ Assumes every state has the same possible actions.
    """

    def __init__(self, q_func, options=None):
        """ e-greedy policy based on the supplied q_table.

        :param q_func: Approximates q values for state action pairs so we can select the best action.
        :param options: can contain:
            'epsilon': small epsilon for the e-greedy policy. This is the probability that we'll
                       randomly select an action, rather than picking the best.
            'epsilon_decay_episodes': the number of episodes over which to decay epsilon
            'epsilon_min': the min value epsilon can be after decay
        """
        self.options = Options(options)
        self.options.default('epsilon', 0.1)

        self.q_func = q_func
        self.epsilon = self.options.get('epsilon')
        self.possible_actions = self.q_func.actions
        if self.options.get('epsilon_decay_episodes') is None:
            self.epsilon_min = self.epsilon
            self.epsilon_decay = 0
        else:
            self.epsilon_min = self.options.get('epsilon_min', 0)
            # We can have multiple workers performing episodes, so number of episodes is distributed, however, we use
            # the number as the number of episodes generated by this worker.
            num_episodes = self.options.get('epsilon_decay_episodes')
            self.epsilon_decay = (self.epsilon - self.epsilon_min) / num_episodes

    def select_action(self, state):
        """ The EGreedy policy selects the best action the majority of the time. However, a random action is chosen
        explore_probability amount of times.

        :param state: The sate to pick an action for
        :return: selected action
        """
        # Select an action for the state, use the best action most of the time.
        # However, with probability of explore_probability, select an action at random.
        if np.random.uniform() < self.epsilon:
            action = self.random_action()
        else:
            action = self.q_func.best_action_for(state)

        return action

    def random_action(self):
        return random.choice(self.possible_actions)

    def decay_epsilon(self):
        # decay epsilon for next time - needs to be called at the end of an episode.
        if self.epsilon > self.epsilon_min:
            self.epsilon = max(self.epsilon - self.epsilon_decay, self.epsilon_min)


class FunctionApprox:

    def __init__(self, options):
        self.options = Options(options)

        self.actions = self.options.get('actions')
        self.q_hat = self.build_neural_network()
        self.q_hat_target = self.build_neural_network()
        self.synch_value_and_target_weights()
        self.batch = []

        self.work_dir = self.options.get('work_dir', self.options.get('env_name'))
        if self.work_dir is not None:
            # location for files.
            self.work_dir = Path(self.work_dir)
            if not self.work_dir.exists():
                self.work_dir.mkdir(parents=True, exist_ok=True)

    def save_weights(self):
        if self.options.get('save_weights', False) and self.work_dir is not None:
            weights_file_name = self.options.get('weights_file', f"{self.options.get('env_name')}.h5")
            weights_file_name = Path(weights_file_name).name
            weights_file = self.work_dir / weights_file_name
            self.q_hat.save_weights(weights_file)

    def load_weights(self):
        if self.options.get('load_weights', False) and self.work_dir is not None:
            weights_file_name = self.options.get('weights_file', f"{self.options.get('env_name')}.h5")
            weights_file_name = Path(weights_file_name).name
            weights_file = self.work_dir / weights_file_name
            if weights_file.exists():
                self.q_hat.load_weights(weights_file)
                self.q_hat_target.load_weights(weights_file)

    def synch_value_and_target_weights(self):
        # Copy the weights from action_value network to the target action_value network
        value_weights = self.q_hat.get_weights()
        target_weights = self.q_hat_target.get_weights()
        # TODO : consider moving towards the value weights, rather than a complete replacement.
        sync_tau = self.options.get('sync_tau', 1.0)
        for i in range(len(target_weights)):
            target_weights[i] = (1 - sync_tau) * target_weights[i] + sync_tau * value_weights[i]
        self.q_hat_target.set_weights(target_weights)

    def get_value_network_weights(self):
        return self.q_hat.get_weights()

    def set_value_network_weights(self, weights):
        self.q_hat.set_weights(weights)

    def get_value_network_checksum(self):
        return self.weights_checksum(self.q_hat.get_weights())

    def get_target_network_weights(self):
        return self.q_hat_target.get_weights()

    def set_target_network_weights(self, weights):
        self.q_hat_target.set_weights(weights)

    def get_target_network_checksum(self):
        return self.weights_checksum(self.q_hat_target.get_weights())

    def weights_checksum(self, weights):
        checksum = 0.0
        for layer_weights in weights:
            checksum += layer_weights.sum()
        return checksum

    def build_neural_network(self):
        # Crete CNN model to predict actions for states.

        try:
            # TODO : give all the layers and models names to indicate worker / controller ?

            inputs = Input(self.options.get('observation_shape'))
            dense_1 = Dense(64, activation='relu')(inputs)
            dense_2 = Dense(64, activation='relu')(dense_1)
            outputs = Dense(len(self.actions), activation='linear')(dense_2)
            network = Model(inputs=inputs, outputs=outputs)

            network.summary()

            # compile the model
            adam_learning_rate = self.options.get('adam_learning_rate', 0.0001)
            network.compile(loss=Huber(delta=1.0), optimizer=Adam(learning_rate=adam_learning_rate))

            return network
        except Exception as e:
            print(f"failed to create model : {e}")

    def get_value(self, state, action):
        prediction = self.q_hat.predict_on_batch(np.expand_dims(state, axis=0))
        return prediction[0][action]

    def get_all_action_values(self, states):
        return self.q_hat.predict_on_batch(states)

    def get_target_value(self, state, action):
        prediction = self.q_hat_target.predict_on_batch(np.expand_dims(state, axis=0))
        return prediction[0][action]

    def get_max_target_value(self, state):
        prediction = self.q_hat_target.predict_on_batch(np.expand_dims(state, axis=0))
        return max(prediction[0])

    def get_max_target_values(self, states):
        predictions = self.q_hat_target.predict_on_batch(states)
        return predictions.max(axis=1)

    def best_action_for(self, state):
        prediction = self.q_hat.predict_on_batch(np.expand_dims(state, axis=0))
        return np.argmax(prediction[0])

    def update_batch(self, batch):
        # do the update in batches
        states = np.array([s for (s, new_action_value) in batch])
        new_action_values = np.array([new_action_value for (s, new_action_value) in batch])

        return self.q_hat.train_on_batch(states, new_action_values)


class AsyncQLearnerWorker(mp.Process):

    def __init__(self, messages, network_gradient_queue, info_queue, options=None):
        super().__init__()
        # messages is a mp.Queue used to receive messages from the controller
        self.messages = messages
        self.network_gradient_queue = network_gradient_queue
        self.info_queue = info_queue
        self.options = Options(options)

        # set these up at the start of run. Saves them being pickled/reloaded when the new process starts.
        self.tasks = None
        self.q_func = None
        self.policy = None
        self.initialised = False
        self.num_lives = 0

    def get_latest_tasks(self):
        """ Get the latest tasks from the controller.

        Overrides any existing task of the same name.
        """

        read_messages = True
        while read_messages:
            try:
                msg_code, content = self.messages.get(False)
                # print(f"worker {self.pid}: got message {msg_code}")
                if msg_code in self.tasks:
                    self.tasks[msg_code] = content
                else:
                    print(f"worker {self.pid}: ignored unrecognised message {msg_code}")
            except Empty:
                # Nothing to process, so just carry on
                read_messages = False
            except Exception as e:
                print(f"read messages failed")
                print(e)

    def process_controller_messages(self):
        """ see if there are any messages from the controller, and process accordingly
        """
        self.get_latest_tasks()
        if self.tasks['stop']:
            return

        if self.tasks['value_network_weights'] is not None:
            # print(f"worker {self.pid}: update the value network weights")
            # print(f"Worker {self.pid} : update value network weights "
            #       f"{self.q_func.weights_checksum(self.tasks['value_network_weights'])}")
            self.q_func.set_value_network_weights(self.tasks['value_network_weights'])
            self.tasks['value_network_weights'] = None

        if self.tasks['target_network_weights'] is not None:
            # print(f"worker {self.pid}: update the target network weights")
            # print(f"Worker {self.pid} : update target network weights "
            #       f"{self.q_func.weights_checksum(self.tasks['target_network_weights'])}")
            self.q_func.set_target_network_weights(self.tasks['target_network_weights'])
            self.tasks['target_network_weights'] = None

        if self.tasks['reset_network_weights'] is not None:
            # print(f"worker {self.pid}: reset both value and target network weights")
            network_weights = self.tasks['reset_network_weights']
            # print(f"Worker {self.pid} : reset value and target network weights "
            #       f"{self.q_func.weights_checksum(network_weights)}")
            self.q_func.set_value_network_weights(network_weights)
            self.q_func.set_target_network_weights(network_weights)

            self.initialised = True
            self.tasks['reset_network_weights'] = None

    def process_batch(self, state_action_batch):
        """ Process the batch and update the q_func, value function approximation.

        :param state_action_batch: List of tuples with state, action, reward, next_state, terminated
        """
        # process the batch - get the values and target values in single calls
        # TODO : make this neater - vector based?
        states = []
        next_states = []
        actions = []
        rewards = []
        for data_item in state_action_batch:
            states.append(data_item[0])
            next_states.append(data_item[3])
            actions.append(data_item[1])
            rewards.append(data_item[2])
        states = np.array(states)
        next_states = np.array(next_states)

        qsa_action_values = self.q_func.get_all_action_values(states)
        next_state_action_values = self.q_func.get_max_target_values(next_states)
        discounted_next_qsa_values = self.options.get('discount_factor') * next_state_action_values
        updates = []
        min_q = None
        max_q = None
        min_q_for_a = None
        max_q_for_a = None
        # weights = self.q_func.get_value_network_weights()
        # print(f"worker {self.pid} : weights[0][0][0][0][0:5] {weights[0][0][0][0][0:5]}")
        # print(f"worker {self.pid} : weights[1][0] {weights[1][0]}")
        #
        # print(f"worker {self.pid} : a={actions} : r={rewards} : qsa={qsa_action_values} : qsa_next={discounted_next_qsa_values}")
        for data_item, qsa_action_value, discounted_next_qsa_value in zip(state_action_batch, qsa_action_values, discounted_next_qsa_values):
            s, a, r, next_s, terminated = data_item
            if terminated:
                y = r
            else:
                y = r + discounted_next_qsa_value

            if min_q_for_a is None:
                min_q_for_a = qsa_action_value[a]
                max_q_for_a = qsa_action_value[a]
            else:
                min_q_for_a = min(qsa_action_value[a], min_q_for_a)
                max_q_for_a = max(qsa_action_value[a], max_q_for_a)
            if min_q is None:
                min_q = qsa_action_value.min()
                max_q = qsa_action_value.max()
            else:
                min_q = min(qsa_action_value.min(), min_q)
                max_q = max(qsa_action_value.max(), max_q)

            # update the action value to move closer to the target
            qsa_action_value[a] = y

            updates.append((s, qsa_action_value))

        losses = self.q_func.update_batch(updates)
        return losses, min_q_for_a, max_q_for_a, min_q, max_q

    def run(self):
        print(f"I am a worker, my PID is {self.pid}")

        self.options.default('async_update_freq', 5)
        self.options.default('discount_factor', 0.99)

        # keep track of requests from the controller by recording them in the tasks dict.
        self.tasks = {
            'value_network_weights': None,
            'target_network_weights': None,
            'reset_network_weights': None,
            'stop': None
        }

        self.q_func = FunctionApprox(self.options)

        # TODO : add epsilon to options
        self.policy = EGreedyPolicy(self.q_func, self.options)
        print(f"worker {self.pid}: Created policy epsilon={self.policy.epsilon}, "
              f"min={self.policy.epsilon_min}, decay={self.policy.epsilon_decay}")

        self.initialised = False

        steps = 0
        steps_since_async_update = 0

        register_gym_mods()
        env = gym.make(self.options.get('env_name'))
        terminated = True
        action = -1
        total_undiscounted_reward = 0
        state_action_buffer = []
        min_q_value_for_a, max_q_value_for_a = None, None
        min_q_value, max_q_value = None, None

        while True:

            self.process_controller_messages()

            if self.tasks['stop']:
                print(F"Worker {self.pid} stopping due to request from controller")
                return

            if len(state_action_buffer) >= self.options.get('async_update_freq'):
                # We've processed enough steps to do an update.
                weights_before = self.q_func.get_value_network_weights()
                loss, min_q_for_a, max_q_for_a,  min_q, max_q = self.process_batch(state_action_buffer)
                temp = {
                    'min_q_for_a': min_q_for_a,
                    'max_q_for_a': max_q_for_a,
                    'min_q': min_q,
                    'max_q': max_q,
                    'min_q_value_for_a': min_q_value_for_a,
                    'max_q_value_for_a': max_q_value_for_a,
                    'min_q_value': min_q_value,
                    'max_q_value': max_q_value,
                }
                if min_q_value_for_a is None:
                    min_q_value_for_a = min_q_for_a
                    max_q_value_for_a = max_q_for_a
                else:
                    min_q_value_for_a = min(min_q_for_a, min_q_value_for_a)
                    max_q_value_for_a = max(max_q_for_a, max_q_value_for_a)
                if min_q_value is None:
                    min_q_value = min_q
                    max_q_value = max_q
                else:
                    min_q_value = min(min_q, min_q_value)
                    max_q_value = max(max_q, max_q_value)
                weights_after = self.q_func.get_value_network_weights()
                weight_deltas = [w_after - w_before for w_before, w_after in zip(weights_before, weights_after)]
                # send the gradient deltas back to the controller
                try:
                    # print(f"worker about to send weights deltas to controller. steps {steps}")
                    self.network_gradient_queue.put((len(state_action_buffer), weight_deltas))
                except Exception as e:
                    print(f"worker failed to send weight deltas")
                    print(e)
                state_action_buffer = []

            if self.initialised:
                if terminated:
                    state, info = env.reset()

                    action = self.policy.select_action(state)

                    total_undiscounted_reward = 0
                    terminated = False

                else:
                    # Take action A, observe R, S'
                    next_state, reward, terminated, truncated, info = env.step(action)
                    terminated = terminated or truncated    # treat truncated as terminated

                    # Choose A' from S' using policy derived from q_func
                    next_action = self.policy.select_action(next_state)

                    state_action_buffer.append((state, action, reward, next_state, terminated))

                    steps += 1
                    steps_since_async_update += 1

                    total_undiscounted_reward += reward
                    if terminated:
                        try:
                            # print(f"send info message from worker")
                            # print(f"worker about to send weights deltas to controller")
                            self.info_queue.put({
                                'total_reward': total_undiscounted_reward,
                                'steps': steps,
                                'worker': self.pid,
                                'epsilon': self.policy.epsilon,
                                'min_q_value_for_a': min_q_value_for_a,
                                'max_q_value_for_a': max_q_value_for_a,
                                'min_q_value': min_q_value,
                                'max_q_value': max_q_value,
                                'value_network_checksum': self.q_func.get_value_network_checksum(),
                                'target_network_checksum': self.q_func.get_target_network_checksum()
                            })
                        except Exception as e:
                            print(f"worker failed to send weight deltas")
                            print(e)
                        # print(f"Worker {self.pid}: Finished episode after {steps} steps. "
                        #       f"Total reward {total_undiscounted_reward}")
                        steps = 0
                        min_q_value, max_q_value = None, None
                        min_q_value_for_a, max_q_value_for_a = None, None
                        # apply epsilon decay after each episode
                        self.policy.decay_epsilon()

                    state, action = next_state, next_action


class AsyncStatsCollector(mp.Process):
    """ Gather some stats for the controller when asked to do so.

    Plays a game against the environment using the supplied weights.
    """

    def __init__(self, messages, options=None):
        super().__init__()
        # messages is a mp.Queue used to receive messages from the controller
        self.messages = messages
        self.options = Options(options)

        # set these up at the start of run. Saves them being pickled/reloaded when the new process starts.
        self.tasks = None
        self.q_func = None
        self.policy = None
        self.num_lives = 0
        self.best_reward = None
        self.best_episode = 0
        self.last_n_scores = None
        self.stats = None
        self.work_dir = None
        self.stats_file = None
        self.info_file = None

    def get_latest_tasks(self):
        """ Get the latest tasks from the controller.

        Overrides any existing task of the same name.
        """

        read_messages = True
        while read_messages:
            try:
                msg_code, content = self.messages.get(False)
                # print(f"worker {self.pid}: got message {msg_code}")
                if msg_code in self.tasks:
                    self.tasks[msg_code] = content
                else:
                    print(f"worker {self.pid}: ignored unrecognised message {msg_code}")
            except Empty:
                # Nothing to process, so just carry on
                read_messages = False
            except Exception as e:
                print(f"read messages failed")
                print(e)

    def process_controller_messages(self, env):
        """ see if there are any messages from the controller, and process accordingly
        """
        self.get_latest_tasks()
        if self.tasks['stop']:
            return

        if self.tasks['play'] is not None:
            # print(f"worker {self.pid}: update the value network weights")
            contents = self.tasks['play']
            self.tasks['play'] = None
            # print(f"stats_collector: update weights play  {self.q_func.weights_checksum(contents['weights'])}")
            self.q_func.set_value_network_weights(contents['weights'])
            updated_weights = self.q_func.get_value_network_weights()
            # print(f"stats_collector: updated weights play  {self.q_func.weights_checksum(updated_weights)}")

            episode = contents['episode_count']
            play_rewards = []
            while len(play_rewards) <  self.options.get('play_avg', 2):
                play_rewards.append(self.play(env, episode))

            avg_play_reward = sum(play_rewards) / len(play_rewards)
            self.stats.append((episode, avg_play_reward))
            with open(self.stats_file, 'a') as file:
                file.write(f"{episode}, {round(avg_play_reward,2):0.2f}\n")
            self.plot_rewards()

    def plot_rewards(self):
        # Plot a graph showing reward against episode, and epsilon
        x = [stat[0] for stat in self.stats]
        rwd = [stat[1] for stat in self.stats]

        fig, ax = plt.subplots()

        color = 'tab:blue'
        ax.set_title(self.options.get('plot_title', 'Asynch DQN rewards'))
        ax.set_xlabel('episodes')
        ax.set_ylabel('reward', color=color)
        # ax1.set_ylim(0, 50)
        ax.plot(x, rwd, color=color)
        ax.tick_params(axis='y', labelcolor=color)

        fig.tight_layout()

        plt.savefig(self.work_dir / 'async_rewards.pdf')
        plt.savefig(self.work_dir / 'async_rewards.png')
        plt.close('all')    # matplotlib holds onto all the figures if we don't close them.

    def play(self, env, episode_count):
        """ play a single episode using a greedy policy

        :param env: The env to use to run an episode
        :param episode_count: Number of episodes that have been run by the workers to generate the policy
        :return: The reward returned by using the policy against the env.
        """
        # print('stats_collector: play')

        total_reward = 0

        state, info = env.reset()

        terminated = False
        steps = 0
        last_action = -1
        repeated_action_count = 0
        action_frequency = {a: 0 for a in self.options.get('actions')}

        while not terminated:
            action = self.policy.select_action(state)
            if action == last_action:
                repeated_action_count += 1
                # check it doesn't get stuck
                if repeated_action_count > 1000:
                    print(f"Play with greedy policy has probably got stuck - action {action} repeated 1000 times")
                    break
            else:
                repeated_action_count = 0

            action_frequency[action] += 1

            state, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            terminated = terminated or truncated    # treat truncated as terminated

            steps += 1
            if steps >= 100000:
                print(f"Break out as we've taken {steps} steps. Something has probably gone wrong...")
                break

        if self.best_reward is None or total_reward > self.best_reward:
            self.best_reward = total_reward
            self.best_episode = episode_count

        self.last_n_scores.append(total_reward)

        print(f"Play : weights from episode {episode_count}"
              f" : Reward = {total_reward}"
              f" : epsilon: {self.policy.epsilon:0.4f}"
              f" : value_net: {self.q_func.get_value_network_checksum():0.4f}"
              f" : Action freq: {action_frequency}"
              f" : Best episode = {self.best_episode} : Best reward = {self.best_reward}"
              f" : Avg reward (last {len(self.last_n_scores)}) = {mean(self.last_n_scores)}")

        with open(self.info_file, 'a') as file:
            file.write(f'{episode_count}, {total_reward}, {self.policy.epsilon}, "{action_frequency}"\n')

        return total_reward

    def run(self):
        print(f"I am a stats collector, my PID is {self.pid}")

        self.options.default('stats_epsilon', 0.05)
        self.last_n_scores = deque(maxlen=5)
        self.stats = []
        self.work_dir = self.options.get('work_dir', self.options.get('env_name'))
        # location for files.
        self.work_dir = Path(self.work_dir)
        if not self.work_dir.exists():
            self.work_dir.mkdir(parents=True, exist_ok=True)
        time_stamp = time.strftime("%Y%m%d-%H%M%S")
        self.stats_file = self.work_dir / f"rewards-{time_stamp}.csv"
        with open(self.stats_file, 'a') as file:
            file.write(f"episode, reward\n")
        self.info_file = self.work_dir / f"info-{time_stamp}.csv"
        with open(self.info_file, 'a') as file:
            file.write(f"episode, reward, epsilon, value_net_checksum, action_frequency\n")

        # keep track of requests from the controller by recording them in the tasks dict.
        self.tasks = {
            'play': None,
            'stop': None
        }

        self.q_func = FunctionApprox(self.options)
        # use a policy with epsilon of 0.05 for playing to collect stats.
        self.policy = EGreedyPolicy(self.q_func, {'epsilon': self.options.get('stats_epsilon')})
        print(f"stats collector {self.pid}: Created policy epsilon={self.policy.epsilon}, "
              f"min={self.policy.epsilon_min}, decay={self.policy.epsilon_decay}")

        steps = 0
        steps_since_async_update = 0

        register_gym_mods()
        env = gym.make(self.options.get('env_name'))
        terminated = True
        state_and_history = None
        action = -1
        total_undiscounted_reward = 0
        state_action_buffer = []

        while True:

            self.process_controller_messages(env)

            if self.tasks['stop']:
                print(F"stats collector {self.pid} stopping due to request from controller")
                return


class AsyncQLearningController:

    def __init__(self, options=None):
        self.options = Options(options)
        self.q_func = FunctionApprox(self.options)
        self.q_func.load_weights()
        self.workers = []
        self.stats_collector = None

    def message_all_workers(self, msg_code, content):
        msg = (msg_code, content)
        for worker, worker_queue in self.workers:
            # send message to each worker
            # print(f"controller sending : {msg_code}")
            try:
                worker_queue.put(msg)
            except Exception as e:
                print(f"Queue put failed in controller")
                print(e)
            # print(f"controller sent : {msg_code}")

    def message_stats_collector(self, msg_code, content):
        msg = (msg_code, content)
        stats_queue, worker = self.stats_collector
        try:
            # print(f"Send {msg_code} message to stats collector")
            stats_queue.put(msg)
        except Exception as e:
            print(f"stats_queue put failed in controller")
            print(e)

    def update_value_network_weights(self, gradient_deltas):
        value_network_weights = self.q_func.get_value_network_weights()
        # print(f"Controller : value network weights before deltas {self.q_func.weights_checksum(value_network_weights)}")
        for (weights, deltas) in zip(value_network_weights, gradient_deltas):
            weights += deltas

        # print(f"Controller : update value network weights {self.q_func.weights_checksum(value_network_weights)}")
        self.q_func.set_value_network_weights(value_network_weights)

        self.message_all_workers('value_network_weights', value_network_weights)

    def train(self):
        print(f"Controller {os.getppid()}: Setting up workers to run asynch q-learning")
        # Need to make sure workers are spawned rather than forked otherwise keras gets into a deadlock. Which seems
        # to be due to multiple processes trying to share the same default tensorflow graph.
        mp.set_start_method('spawn')

        # TODO : does this need a max size setting?
        network_gradients_queue = mp.Queue()
        info_queue = mp.Queue()

        # set up the workers

        for _ in range(self.options.get('num_workers', 1)):
            worker_queue = mp.Queue()
            worker = AsyncQLearnerWorker(worker_queue, network_gradients_queue, info_queue, self.options)
            worker.daemon = True    # helps tidy up child processes if parent dies.
            worker.start()
            self.workers.append((worker, worker_queue))

        # Send the workers the starting weights
        starting_weights = self.q_func.get_value_network_weights()
        # print(f"Controller : init weights for workers {self.q_func.weights_checksum(starting_weights)}")
        self.message_all_workers('reset_network_weights', starting_weights)

        # set up stats_gatherer
        stats_queue = mp.Queue()
        worker = AsyncStatsCollector(stats_queue, self.options)
        worker.daemon = True    # helps tidy up child processes if parent dies.
        worker.start()
        self.stats_collector = (stats_queue, worker)

        total_steps = 0
        episode_count = 0
        target_sync_counter = self.options.get('target_net_sync_steps', 1)
        best_reward = None
        best_episode = 0

        while episode_count < self.options.get('episodes', 1):

            gradient_messages = []
            try:
                while True:
                    # print(f"controller about to read from network gradients queue")
                    gradient_messages.append(network_gradients_queue.get(False))
                    # print(f"controller got network gradients")

            except Empty:
                # Nothing to process, so just carry on
                pass

            except Exception as e:
                print(f"Failed to get gradients off the queue")
                print(e)

            if len(gradient_messages) > 0:
                # print(f"apply the gradients, count of messages = {len(gradient_messages)}")
                gradient_deltas = None
                for (steps, deltas) in gradient_messages:
                    total_steps += steps
                    target_sync_counter -= steps
                    # update the value network weights.
                    if gradient_deltas is None:
                        gradient_deltas = deltas
                    else:
                        for i in range(len(gradient_deltas)):
                            gradient_deltas[i] += deltas[i]

                if gradient_deltas is not None:
                    if len(gradient_messages) > 1:
                        # multiple gradient messages - need to take an average...
                        # print(f"Multiple gradient messages ({len(gradient_messages)}), so take an average")
                        # TODO: check how we should handle this
                        for i in range(len(gradient_deltas)):
                            gradient_deltas[i] /= len(gradient_messages)
                    self.update_value_network_weights(gradient_deltas)

            if target_sync_counter <= 0:
                # synch the target weights with the value weights, then let the workers know.
                self.q_func.synch_value_and_target_weights()
                network_weights = self.q_func.get_target_network_weights()
                # print(f"Controller : target network synch {self.q_func.weights_checksum(network_weights)}")
                self.message_all_workers('target_network_weights', network_weights)
                target_sync_counter = self.options.get('target_net_sync_steps', 1)

            try:
                # print(f"controller about to read from info queue")
                info = info_queue.get(False)
                episode_count += 1
                if best_reward is None or  best_reward < info['total_reward']:
                    best_reward = info['total_reward']
                    best_episode = episode_count
                print(f"Total steps {total_steps} : Episode {episode_count} took {info['steps']} steps"
                      f" : reward = {info['total_reward']} : pid = {info['worker']}"
                      f" : value_net = {info['value_network_checksum']:0.4f}"
                      f" : target_net = {info['target_network_checksum']:0.4f}"
                      f" : epsilon = {info['epsilon']:0.4f}"
                      f" : min,max q = {info['min_q_value']:0.2f},{info['max_q_value']:0.2f}"
                      f" : min,max q* = {info['min_q_value_for_a']:0.2f},{info['max_q_value_for_a']:0.2f}"
                      f" : best_reward (episode) = {best_reward} ({best_episode})")

                if episode_count % self.options.get('stats_every') == 0:
                    self.q_func.save_weights()
                    weights = self.q_func.get_value_network_weights()
                    self.message_stats_collector('play', {'weights': weights, 'episode_count': episode_count})

            except Empty:
                # Nothing to process, so just carry on
                pass

            except Exception as e:
                print(f"Failed to read info_queue")
                print(e)

        # close down the workers
        print(f"All done, {total_steps} steps processed - close down the workers")
        self.message_all_workers('stop', True)
        self.message_stats_collector('stop', True)

        self.q_func.save_weights()

        time.sleep(5)   # give them a chance to stop
        try:
            # make sure they've all stopped OK
            for worker, _ in self.workers:
                worker.terminate()
                worker.join(5)
            stats_queue, stats_worker = self.stats_collector
            stats_worker.terminate()
            stats_worker.join(5)
        except Exception as e:
            print("Failed to close the workers cleanly")
            print(e)

def register_gym_mods():
    gym.envs.registration.register(
        id='MountainCarMyEasyVersion-v0',
        entry_point='gym.envs.classic_control.mountain_car:MountainCarEnv',
        max_episode_steps=250,      # MountainCar-v0 uses 200
        reward_threshold=-110.0
    )

def create_and_run_agent():

    timer = Timer()
    timer.start("agent")
    agent = AsyncQLearningController(options={
        # 'env_name': "Acrobot-v1",
        # 'observation_shape': (6, ),
        # 'actions': [0, 1, 2],
        # 'env_name': "MountainCar-v0",
        # 'env_name': "MountainCarMyEasyVersion-v0",
        # 'observation_shape': (2, ),
        # 'actions': [0, 1, 2],
        'env_name': "CartPole-v1",
        # 'render': "human",
        'observation_shape': (4, ),
        'actions': [0, 1],
        'num_workers': 1,
        'episodes': 500,
        'async_update_freq': 4,
        'target_net_sync_steps': 50,
        'sync_tau': 0.5,
        'adam_learning_rate': 0.001,
        'discount_factor': 0.85,
        'load_weights': False,
        'save_weights': True,
        'stats_epsilon': 0.01,
        'epsilon': 0.75,
        'epsilon_min': 0.1,
        'epsilon_decay_episodes': 350,
        'stats_every': 10
    })

    agent.train()
    timer.stop('agent')
    elapsed = timer.event_times['agent']
    hours = int(elapsed / 3600)
    elapsed = elapsed - 3600*hours
    mins = int(elapsed / 60)
    secs = int(elapsed - 60*mins)
    print(f"Total time taken by agent is {hours} hours {mins} mins {secs} secs")

if __name__ == '__main__':
    create_and_run_agent()